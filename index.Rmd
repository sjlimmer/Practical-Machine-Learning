---
title: "Machine Learning Final Project"
author: "sjlimmer"
date: "May 19, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```

## Executive summary

Data from six accelerometers is used to try and predict how well a group of 6 participants
performed a specified exercise activity.

More information is available from the website here: <http://groupware.les.inf.puc-rio.br/har>
(see the section on the Weight Lifting Exercise Dataset). 

### Data

The training data for this project are 
available here:

<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>

The test data are available here:

<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>

```{r, cache=T}
if(!file.exists('pml-training.csv') & !file.exists('pml-testing.csv')) {
    download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv',
                  'pml-training.csv')
        download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv',
                  'pml-testing.csv')
    downldTime <- Sys.time()
}
training <- read.csv('pml-training.csv', header=T, na.strings=c('NA', '#DIV/0!'))
testing <- read.csv('pml-testing.csv', header=T, na.strings=c('NA', '#DIV/0!'))
```

### Analysis

In reading the data I have treated '#DIV/0!' as an additional NA string. Many (~100) of the predictor
variables are majority (>97%) filled with NAs. Processing the training data to remove these columns
makes for an easier analysis.

```{r}
training2 <- training[,!apply(is.na(training), 2, any)]
```


In order to develop an accurate model, I have used repeated k-fold cross validation, with
10 folds and 3 repeats. THis is combined with a random forest method, chosen for it's high accuracy.
The speed of this analysis is slower than perhaps would be ideal, but was sufficient for the 
analysis.

```{r, cache=T}
library(caret)

## 10-fold CV, repeated three times
fitControl <- trainControl(method = "repeatedcv",number = 10,repeats = 3)
set.seed(2305)
classeMdl <- train(classe~., data=training2, method='rf', trControl=fitControl)
classeMdl$finalModel
predclasse <- predict(classeMdl, training2)
confusionMatrix(predclasse, training2$classe)

```
Based on the reported accuracy (1) and OOB error estimate (0.01%), I would estimate the 
out of sample error is < 1%. I am somewhat concerned that this model may have overfit the
data given the high accuracy, in which case the error on the test set may be higher.
